model: ConvSeq2Seq
model_params:
  attention.class: seq2seq.decoders.attention.AttentionLayerDot 
  attention.params:
    num_units: 32
  embedding.dim: 32
  encoder.class: seq2seq.encoders.ConvEncoderFairseq
  encoder.params:
    cnn.layers: 2
    cnn.nhids: 32,32
    cnn.kwidths: 10,10
  decoder.class: seq2seq.decoders.ConvDecoderFairseq
  decoder.params:
    cnn.layers: 1
    cnn.nhids: 32
    cnn.kwidths: 4
  optimizer.name: Adam
  optimizer.params:
    epsilon: 0.0000008
  optimizer.learning_rate: 0.001
  source.max_seq_len: 66
  source.reverse: false
  target.max_seq_len: 5
